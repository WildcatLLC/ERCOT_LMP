{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect with ERCOT Public API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import zipfile\n",
    "import shutil\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Third-party imports\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Pull Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_ercot_token():\n",
    "    # Get credentials from environment variables\n",
    "    username = os.getenv(\"ERCOT_USERNAME\")\n",
    "    password = os.getenv(\"ERCOT_PASSWORD\")\n",
    "    subscription_key = os.getenv(\"ERCOT_SUBSCRIPTION_KEY\")\n",
    "    \n",
    "    # Check if credentials are available\n",
    "    if not all([username, password, subscription_key]):\n",
    "        raise ValueError(\"Missing required environment variables. Please check your .env file.\")\n",
    "    \n",
    "    # Authentication URL\n",
    "    auth_url = \"https://ercotb2c.b2clogin.com/ercotb2c.onmicrosoft.com/B2C_1_PUBAPI-ROPC-FLOW/oauth2/v2.0/token\"\n",
    "    \n",
    "    # Request data as form data\n",
    "    data = {\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\",\n",
    "        \"scope\": \"openid fec253ea-0d06-4272-a5e6-b478baeecd70 offline_access\",\n",
    "        \"client_id\": \"fec253ea-0d06-4272-a5e6-b478baeecd70\",\n",
    "        \"response_type\": \"id_token\"\n",
    "    }\n",
    "    \n",
    "    # Make POST request with form data\n",
    "    response = requests.post(auth_url, data=data)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        # Add subscription key to token data for convenience\n",
    "        token_data['subscription_key'] = subscription_key\n",
    "        return token_data\n",
    "    else:\n",
    "        raise Exception(f\"Authentication failed with status code {response.status_code}: {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Token Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    token_data = get_ercot_token()\n",
    "    print(\"Authentication successful!\")\n",
    "    print(f\"Access token: {token_data['access_token'][:20]}...\")\n",
    "    print(f\"ID token: {token_data['id_token'][:20]}...\")\n",
    "    print(f\"Token expires in: {token_data['expires_in']} seconds\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request LMP Data Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_history_bundles(token_data, page):\n",
    "    \"\"\"\n",
    "    Retrieve product history from ERCOT API\n",
    "    \n",
    "    Args:\n",
    "        token_data: The authentication token data\n",
    "        \n",
    "    Returns:\n",
    "        Product history data\n",
    "    \"\"\"\n",
    "    # Base URL for getProductHistory endpoint\n",
    "    base_url = \"https://api.ercot.com/api/public-reports/archive/np6-787-cd\"\n",
    "    \n",
    "    # Add date range parameters\n",
    "    params = {\n",
    "        \"postDatetimeFrom\": \"2024-01-01\",\n",
    "        \"postDatetimeTo\": \"2024-12-31\",\n",
    "        \"page\": page\n",
    "    }\n",
    "    \n",
    "    # Headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token_data['access_token']}\",\n",
    "        \"Ocp-Apim-Subscription-Key\": token_data['subscription_key']\n",
    "    }\n",
    "    \n",
    "    # Make the request with parameters\n",
    "    response = requests.get(base_url, headers=headers, params=params, timeout=30)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        \n",
    "        # Return the results\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        else:\n",
    "            return [data]\n",
    "    else:\n",
    "        raise Exception(f\"Request failed with status code {response.status_code}: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call LMP Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Make sure you have a valid token\n",
    "try:\n",
    "    # token_data = get_ercot_token()\n",
    "    print(\"Authentication successful!\")\n",
    "    print(f\"Access token: {token_data['access_token'][:20]}...\")\n",
    "\n",
    "    # Step 2: Initialize an empty list to store all bundles\n",
    "    all_bundles = []\n",
    "\n",
    "    # Step 3: Get the first page to determine total pages\n",
    "    first_page = get_product_history_bundles(token_data, page=1)\n",
    "    if first_page and len(first_page) > 0:\n",
    "        total_pages = first_page[0][\"_meta\"][\"totalPages\"]\n",
    "        current_page = first_page[0][\"_meta\"][\"currentPage\"]\n",
    "\n",
    "        # Add the first page data to our collection\n",
    "        all_bundles.extend(first_page)\n",
    "        print(f\"Retrieved page {current_page} of {total_pages}\")\n",
    "\n",
    "        # Step 4: Iterate through remaining pages with improved rate limiting\n",
    "        import time\n",
    "        request_count = 1  # Already made one request for the first page\n",
    "        request_start_time = time.time()\n",
    "\n",
    "        while current_page < total_pages:\n",
    "            current_page += 1\n",
    "\n",
    "            # Rate limiting: More conservative approach - max 25 requests per minute\n",
    "            if request_count >= 25:\n",
    "                elapsed = time.time() - request_start_time\n",
    "                if elapsed < 60:\n",
    "                    sleep_time = 60 - elapsed + 5  # Add 5 second buffer\n",
    "                    print(f\"Rate limit approaching. Pausing for {sleep_time:.2f} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                request_count = 0\n",
    "                request_start_time = time.time()\n",
    "\n",
    "            # Add exponential backoff for 429 errors\n",
    "            max_retries = 5\n",
    "            retry_count = 0\n",
    "            retry_delay = 2  # Start with 2 seconds\n",
    "\n",
    "            while retry_count < max_retries:\n",
    "                try:\n",
    "                    # Directly pass the page parameter without modifying token_data\n",
    "                    page_data = get_product_history_bundles(token_data, page=current_page)\n",
    "                    request_count += 1\n",
    "\n",
    "                    if page_data:\n",
    "                        all_bundles.extend(page_data)\n",
    "                        print(f\"Retrieved page {current_page} of {total_pages}\")\n",
    "                    break  # Success, exit retry loop\n",
    "                except Exception as e:\n",
    "                    if \"429\" in str(e):  # Rate limit error\n",
    "                        retry_count += 1\n",
    "                        if retry_count < max_retries:\n",
    "                            print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "                            time.sleep(retry_delay)\n",
    "                            retry_delay *= 2  # Exponential backoff\n",
    "                            request_count = 0\n",
    "                            request_start_time = time.time()\n",
    "                        else:\n",
    "                            print(f\"Max retries reached. Skipping page {current_page}.\")\n",
    "                            break\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            time.sleep(0.5)  # Small delay between requests\n",
    "\n",
    "    # Step 5: Process the results - process all collected data\n",
    "    if all_bundles:\n",
    "        print(f\"Successfully retrieved data across {len(all_bundles)} total bundles\")\n",
    "\n",
    "        # Optional: Save to a file\n",
    "        with open(\"ercot_product_bundles.json\", \"w\") as f:\n",
    "            json.dump(all_bundles, f, indent=2)\n",
    "        print(\"Saved results to ercot_product_bundles.json\")\n",
    "\n",
    "        # Optional: Convert to DataFrame for analysis\n",
    "        import pandas as pd\n",
    "        flattened_data = []\n",
    "        for bundle in all_bundles:\n",
    "            if \"archives\" in bundle:\n",
    "                for archive in bundle[\"archives\"]:\n",
    "                    record = {\n",
    "                        \"product_name\": bundle.get(\"product\", {}).get(\"name\", \"\"),\n",
    "                        \"emilId\": bundle.get(\"product\", {}).get(\"emilId\", \"\"),\n",
    "                        \"docId\": archive.get(\"docId\", \"\"),\n",
    "                        \"friendlyName\": archive.get(\"friendlyName\", \"\"),\n",
    "                        \"postDatetime\": archive.get(\"postDatetime\", \"\"),\n",
    "                        \"download_url\": archive.get(\"_links\", {}).get(\"endpoint\", {}).get(\"href\", \"\")\n",
    "                    }\n",
    "                    flattened_data.append(record)\n",
    "\n",
    "        df = pd.DataFrame(flattened_data if flattened_data else all_bundles)\n",
    "        print(f\"Created DataFrame with {len(df)} rows\")\n",
    "        print(df.head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save LMP docIds as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "if 'df' in locals() and not df.empty:\n",
    "    csv_filename = \"ercot_product_bundles.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Saved DataFrame to {csv_filename}\")\n",
    "else:\n",
    "    print(\"No DataFrame available to save\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Number of Unique docIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique docIds are in the DataFrame\n",
    "if 'df' in locals() and not df.empty:\n",
    "    unique_doc_ids = df['docId'].nunique()\n",
    "    print(f\"Number of unique docIds in the DataFrame: {unique_doc_ids}\")\n",
    "    \n",
    "    # Optional: Display a few unique docIds as examples\n",
    "    print(\"\\nSample of unique docIds:\")\n",
    "    print(df['docId'].unique()[:5])  # Show first 5 unique docIds\n",
    "else:\n",
    "    print(\"No DataFrame available to analyze unique docIds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download LMP Data as ZIP in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_download_archives(token_data, doc_ids_batch, emil_id=\"np6-787-cd\"):\n",
    "    \"\"\"\n",
    "    Download product archives in bulk using the ERCOT API.\n",
    "    \n",
    "    This function sends a POST request to the bulk download endpoint with a JSON payload\n",
    "    containing up to 1000 docIds.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.ercot.com/api/public-reports/archive/{emil_id}/download\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token_data['access_token']}\",\n",
    "        \"Ocp-Apim-Subscription-Key\": token_data['subscription_key'],\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"docIds\": doc_ids_batch}\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Bulk download failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "# Main processing code\n",
    "try:\n",
    "    # Retrieve your token data before proceeding\n",
    "    token_data = get_ercot_token()\n",
    "    print(\"Authentication successful!\")\n",
    "    print(f\"Access token: {token_data['access_token'][:20]}...\")\n",
    "\n",
    "    # Read CSV file containing docIds (assumes a column named \"docId\")\n",
    "    csv_file = \"ercot_product_bundles.csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        raise Exception(f\"CSV file {csv_file} does not exist.\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    if \"docId\" not in df.columns:\n",
    "        raise Exception(\"CSV file must contain a 'docId' column.\")\n",
    "    \n",
    "    # Extract the list of docIds\n",
    "    doc_ids = df[\"docId\"].dropna().tolist()\n",
    "    total_doc_ids = len(doc_ids)\n",
    "    print(f\"Total docIds to process: {total_doc_ids}\")\n",
    "\n",
    "    # Define batch and rate limit parameters\n",
    "    batch_size = 1000\n",
    "    max_requests_per_minute = 25  # adjust as needed\n",
    "    request_count = 0\n",
    "    request_start_time = time.time()\n",
    "\n",
    "    # If resuming from a specific batch (e.g. after 40 batches are already processed), set starting_index accordingly\n",
    "    starting_index = 91000  # Change to 92 * batch_size if you want to resume from batch 92\n",
    "\n",
    "    # Ensure the download directory exists\n",
    "    download_dir = \"bulk_downloads\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # Process docIds in batches\n",
    "    for batch_index in range(starting_index, total_doc_ids, batch_size):\n",
    "        batch_doc_ids = doc_ids[batch_index: batch_index + batch_size]\n",
    "        batch_number = (batch_index // batch_size) + 1\n",
    "        print(f\"\\nProcessing batch {batch_number} with {len(batch_doc_ids)} docIds...\")\n",
    "\n",
    "        # Rate limiting: Pause if we've reached the max request count\n",
    "        if request_count >= max_requests_per_minute:\n",
    "            elapsed = time.time() - request_start_time\n",
    "            if elapsed < 60:\n",
    "                sleep_time = 60 - elapsed + 5  # 5-second buffer\n",
    "                print(f\"Rate limit reached. Sleeping for {sleep_time:.2f} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "            request_count = 0\n",
    "            request_start_time = time.time()\n",
    "\n",
    "        # Exponential backoff for 429 errors and token expiration handling\n",
    "        max_retries = 5\n",
    "        retry_count = 0\n",
    "        retry_delay = 2\n",
    "\n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                file_content = bulk_download_archives(token_data, batch_doc_ids)\n",
    "                request_count += 1\n",
    "                file_path = os.path.join(download_dir, f\"bulk_download_batch_{batch_number}.zip\")\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(file_content)\n",
    "                print(f\"Successfully downloaded batch {batch_number} to {file_path}\")\n",
    "                break  # Success: exit retry loop\n",
    "\n",
    "            except Exception as e:\n",
    "                error_message = str(e)\n",
    "                # Check for rate limit error (429)\n",
    "                if \"429\" in error_message:\n",
    "                    retry_count += 1\n",
    "                    if retry_count < max_retries:\n",
    "                        print(f\"Rate limit exceeded on batch {batch_number}. Retrying in {retry_delay} seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "                        time.sleep(retry_delay)\n",
    "                        retry_delay *= 2  # Exponential backoff\n",
    "                    else:\n",
    "                        print(f\"Max retries reached for batch {batch_number}. Skipping this batch.\")\n",
    "                        break\n",
    "\n",
    "                # Check for token expiry (commonly a 401 error or an 'expired' message)\n",
    "                elif \"401\" in error_message or \"expired\" in error_message.lower():\n",
    "                    print(f\"Access token appears to be expired during batch {batch_number}. Refreshing token...\")\n",
    "                    token_data = get_ercot_token()\n",
    "                    time.sleep(2)  # Wait briefly before retrying the same batch\n",
    "                    # Do not increment retry_count here so we can reattempt with a fresh token\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    print(f\"Error downloading batch {batch_number}: {error_message}\")\n",
    "                    break\n",
    "\n",
    "        time.sleep(0.5)  # Small delay between batches\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Certain Batches for Re-Try\n",
    "\n",
    "*Used In the Event a Batch Fails to Download and Specific Missing docIDs are the Cause*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_download_archives(token_data, doc_ids_batch, emil_id=\"np6-787-cd\"):\n",
    "    \"\"\"\n",
    "    Download product archives in bulk using the ERCOT API.\n",
    "    \n",
    "    This function sends a POST request to the bulk download endpoint with a JSON payload\n",
    "    containing up to 1000 docIds.\n",
    "    \"\"\"\n",
    "    url = f\"https://api.ercot.com/api/public-reports/archive/{emil_id}/download\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token_data['access_token']}\",\n",
    "        \"Ocp-Apim-Subscription-Key\": token_data['subscription_key'],\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\"docIds\": doc_ids_batch}\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Bulk download failed: {response.status_code} - {response.text}\")\n",
    "\n",
    "\n",
    "# Main processing code\n",
    "try:\n",
    "    # Specify the batch number to download\n",
    "    batch_to_download = 92  # Change this to the specific batch number you want to download\n",
    "    batch_size = 1000\n",
    "    \n",
    "    # Retrieve your token data before proceeding\n",
    "    token_data = get_ercot_token()\n",
    "    print(\"Authentication successful!\")\n",
    "    print(f\"Access token: {token_data['access_token'][:20]}...\")\n",
    "\n",
    "    # Read CSV file containing docIds (assumes a column named \"docId\")\n",
    "    csv_file = \"ercot_product_bundles.csv\"\n",
    "    if not os.path.exists(csv_file):\n",
    "        raise Exception(f\"CSV file {csv_file} does not exist.\")\n",
    "    \n",
    "    df = pd.read_csv(csv_file)\n",
    "    if \"docId\" not in df.columns:\n",
    "        raise Exception(\"CSV file must contain a 'docId' column.\")\n",
    "    \n",
    "    # Extract the list of docIds and exclude specific docIds\n",
    "    excluded_doc_ids = [] # List Excluded docIds Here\n",
    "    doc_ids = df[\"docId\"].dropna().tolist()\n",
    "    doc_ids = [doc_id for doc_id in doc_ids if str(doc_id) not in excluded_doc_ids]\n",
    "    total_doc_ids = len(doc_ids)\n",
    "    print(f\"Total docIds available (after exclusions): {total_doc_ids}\")\n",
    "\n",
    "    # Calculate the starting index for the specified batch\n",
    "    starting_index = (batch_to_download - 1) * batch_size\n",
    "    \n",
    "    # Ensure the batch number is valid\n",
    "    if starting_index >= total_doc_ids:\n",
    "        raise Exception(f\"Batch {batch_to_download} exceeds the available data (max batch: {(total_doc_ids // batch_size) + 1})\")\n",
    "    \n",
    "    # Get the docIds for the specified batch\n",
    "    batch_doc_ids = doc_ids[starting_index: starting_index + batch_size]\n",
    "    print(f\"\\nProcessing batch {batch_to_download} with {len(batch_doc_ids)} docIds...\")\n",
    "\n",
    "    # Ensure the download directory exists\n",
    "    download_dir = \"bulk_downloads\"\n",
    "    os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "    # Exponential backoff for 429 errors and token expiration handling\n",
    "    max_retries = 5\n",
    "    retry_count = 0\n",
    "    retry_delay = 2\n",
    "\n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            file_content = bulk_download_archives(token_data, batch_doc_ids)\n",
    "            file_path = os.path.join(download_dir, f\"bulk_download_batch_{batch_to_download}.zip\")\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(file_content)\n",
    "            print(f\"Successfully downloaded batch {batch_to_download} to {file_path}\")\n",
    "            break  # Success: exit retry loop\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            # Check for rate limit error (429)\n",
    "            if \"429\" in error_message:\n",
    "                retry_count += 1\n",
    "                if retry_count < max_retries:\n",
    "                    print(f\"Rate limit exceeded on batch {batch_to_download}. Retrying in {retry_delay} seconds... (Attempt {retry_count}/{max_retries})\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    retry_delay *= 2  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"Max retries reached for batch {batch_to_download}. Download failed.\")\n",
    "                    break\n",
    "\n",
    "            # Check for token expiry (commonly a 401 error or an 'expired' message)\n",
    "            elif \"401\" in error_message or \"expired\" in error_message.lower():\n",
    "                print(f\"Access token appears to be expired during batch {batch_to_download}. Refreshing token...\")\n",
    "                token_data = get_ercot_token()\n",
    "                time.sleep(2)  # Wait briefly before retrying the same batch\n",
    "                # Do not increment retry_count here so we can reattempt with a fresh token\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                print(f\"Error downloading batch {batch_to_download}: {error_message}\")\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract ZIP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_unzip(folder):\n",
    "    # Create a 'Processed' folder inside the given folder if it doesn't exist\n",
    "    processed_folder = os.path.join(folder, 'Processed')\n",
    "    os.makedirs(processed_folder, exist_ok=True)\n",
    "    \n",
    "    while True:\n",
    "        found_zip = False\n",
    "        # Walk through the directory tree\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            # Skip walking the 'Processed' directory\n",
    "            if 'Processed' in dirs:\n",
    "                dirs.remove('Processed')\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.zip'):\n",
    "                    # Skip any zip files that are already in the Processed folder\n",
    "                    if os.path.abspath(root).startswith(os.path.abspath(processed_folder)):\n",
    "                        continue\n",
    "                    found_zip = True\n",
    "                    zip_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                            zip_ref.extractall(root)\n",
    "                        # Move the processed zip file to the 'Processed' folder\n",
    "                        destination = os.path.join(processed_folder, file)\n",
    "                        shutil.move(zip_path, destination)\n",
    "                        print(f\"Extracted and moved: {zip_path} -> {destination}\")\n",
    "                    except zipfile.BadZipFile:\n",
    "                        print(f\"Warning: '{zip_path}' is not a valid zip file.\")\n",
    "        # If no zip files were found in this pass, exit the loop\n",
    "        if not found_zip:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"bulk_downloads\"  # Replace with the path to your folder\n",
    "    recursive_unzip(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine CSVs into a Single Dataframe and Save the File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_buses = ['AMOCOOIL_CC1'] # Enter in ERCOT ElectricalBuses Here\n",
    "\n",
    "df = dd.read_csv(\"bulk_downloads/*.csv\")\n",
    "df_filtered = df[df[\"ElectricalBus\"].isin(allowed_buses)]\n",
    "# This filter is lazy; Dask wonâ€™t actually load data until you compute or write.\n",
    "df_filtered.to_parquet(\"filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work with Filtered Dataset and Provide 8760(8784) LMPs for Target Buses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load partitioned Parquet dataset\n",
    "df_dask = dd.read_parquet(\"filtered.parquet\")\n",
    "\n",
    "# Convert SCEDTimestamp to datetime format\n",
    "df_dask[\"SCEDTimestamp\"] = dd.to_datetime(df_dask[\"SCEDTimestamp\"])\n",
    "\n",
    "# Round timestamps to the nearest hour\n",
    "df_dask[\"Hour\"] = df_dask[\"SCEDTimestamp\"].dt.floor(\"h\")  # Truncate to start of hour\n",
    "\n",
    "# Group by ElectricalBus and Hour, then calculate average LMP\n",
    "df_grouped = df_dask.groupby([\"ElectricalBus\", \"Hour\"]).agg({\"LMP\": \"mean\"}).reset_index()\n",
    "\n",
    "# Compute the final result (convert from Dask to Pandas)\n",
    "df_result = df_grouped.compute()\n",
    "\n",
    "# Save to Parquet\n",
    "df_result.to_parquet(\"hourly_LMP.parquet\", index=False)\n",
    "\n",
    "# Show first few rows\n",
    "print(df_result.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ERCOT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
